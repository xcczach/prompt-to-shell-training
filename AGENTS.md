## 1) Scope

Implement an end-to-end pipeline that fine-tunes a small LLM (Qwen2.5-0.5B-Instruct or Qwen2.5-0.5B) to convert natural language into **safe, single-line shell commands**. The pipeline must:

* Generate **real + synthetic** training data.
* Train with **Unsloth** (QLoRA) and **automatic multi-GPU** distribution (FSDP).
* Serve **base** and **fine-tuned** models with vLLM for **fair evaluation**.
* Evaluate structure/semantics/functional correctness and safety.

## 2) Output Schema (model response)

Model must emit **one JSON object**:

```json
{
  "type": "bash|zsh|fish|powershell|cmd|nushell",
  "cmd": "<single line command or pipeline>",
  "requires_confirm": true or false,
  "explain": "<brief rationale in the same language as the input>"
}
```

Notes:

* **No `dry_run` field.** Keep only `cmd`, `requires_confirm`, `explain`, plus `type`.
* One command only (pipelines allowed), no multi-line scripts.
* Prefer safe/read-only operations; set `requires_confirm=true` when risky.

## 3) Data Requirements

### 3.1 Sources

* **Real**: curated CLI histories (PII scrubbed), runbooks, READMEs, accepted answers (paraphrased), dotfiles “recipes”.
* **Synthetic**: generated by a teacher LLM from neutral seed text.

### 3.2 Language & OS/shell mix

* Language split: **50% Chinese / 50% English**.
* OS mix: **Linux 45%**, **macOS 25%**, **Windows 30%**.
* Shell mix within OS:

  * Linux: bash 70%, zsh 20%, fish 10%.
  * macOS: zsh 80%, bash 20%.
  * Windows: PowerShell 80%, cmd 20%.
* Allowed `type`: `bash | zsh | fish | powershell | cmd | nushell`. Permit future extension; default to these.

### 3.3 Synthesis prompts (templates)

**system (teacher):**

```
You generate practical, safe terminal tasks. Output a single JSON object with fields:
type, cmd, requires_confirm, explain. Prefer read-only commands. Vary tools and difficulty.
```

**user (teacher):**

```
Context:
<short neutral paragraph or snippet>

Constraints:
- OS distribution target: Linux 45%, macOS 25%, Windows 30%.
- Shell types: bash/zsh/fish on Linux and macOS; PowerShell/cmd on Windows (see ratios).
- One single-line command (pipes ok), no multi-line scripts.
- If operation could be destructive, set requires_confirm=true and prefer a safe variant.
- Produce Chinese for half of tasks, English for the other half.
- Output ONLY the JSON object.
```

### 3.4 Safety filters

* Block patterns: `rm -rf /`, raw `dd` to devices, `mkfs*`, `:(){ :|:& };:`, `shutdown`, credential dumpers, obvious net-attack tools.
* Lint:

  * bash/zsh/fish: `shellcheck` (treat warnings as filterable; errors reject).
  * PowerShell: `pwsh -NoProfile -Command { <cmd> }` syntax check (no execution).
  * cmd: heuristic validation (`/?` help presence, quoting rules).
* Normalize and **deduplicate** by canonicalized `cmd`.

### 3.5 Splits

* Split by **task family** (avoid leakage): train 80%, val 10%, test 10%.
* Keep a **challenge test** with: long pipelines, quoting/escaping, Windows path quirks, macOS Homebrew, find/awk/sed composition.

## 4) Repository Layout (to implement)

```
shell-lm/
  data/
    real/                         # curated real pairs
    synthetic/
      raw_text/                   # seed paragraphs
      prompts/                    # teacher prompt templates
      samples/                    # generated JSONL
    splits/                       # train/val/test jsonl
    eval/                         # test prompts & gold
  configs/
    train_sft.yaml                # Unsloth SFT config
    eval.yaml                     # eval settings
  scripts/
    synth_generate.py             # teacher-driven synthesis
    filter_lint.py                # safety + lint + balance + dedup
    split_dataset.py              # family-aware split
    train_unsloth.py              # Unsloth QLoRA + FSDP
    serve_vllm_base.sh            # vLLM for base model
    serve_vllm_ft.sh              # vLLM for finetuned (adapter or merged)
    eval_generate.py              # query both endpoints
    exec_sandbox.py               # Docker sandbox runner
    eval_score.py                 # metrics & report
  README.md
  AGENTS.md
```

## 5) Implementation Tasks & Acceptance Criteria

### T1) `synth_generate.py`

**Goal:** Produce `data/synthetic/samples/*.jsonl` with desired OS/shell/language ratios.

**Inputs:**

* `--seeds data/synthetic/raw_text/*.txt`
* `--out data/synthetic/samples/synth_v1.jsonl`
* `--target_counts '{"linux":45,"macos":25,"windows":30}'`
* `--teacher_endpoint <http(s)://...>` or local CLI call

**Behavior:**

* Sample seeds; call teacher with templates; enforce ratios with reservoir/quota sampling.
* Generate 2–3 **paraphrases** per NL instruction automatically.
* Write JSONL rows: `{input, context, output_json}` where:

  * `input`: NL instruction (CN/EN)
  * `context`: `{os, shell, hints?}`
  * `output_json`: **string** containing the exact JSON (schema above)

**Done when:** JSONL validates; distribution matches ±1% per bucket; no invalid JSON.

---

### T2) `filter_lint.py`

**Goal:** Enforce safety, linting, balance, dedup.

**Inputs:** `--in <jsonl> --out <jsonl>`

**Behavior:**

* Hard-ban patterns; reject unsafe.
* Lint by shell; drop on syntax error; warn on style.
* Canonicalize command (normalize spacing/option order where safe) → dedup.
* Rebalance to OS/shell/language targets if drifted.

**Done when:** Produces a **clean** JSONL with logs of rejections & reasons.

---

### T3) `split_dataset.py`

**Goal:** Create `data/splits/{train,val,test}.jsonl`.

**Inputs:** `--in <jsonl> --family_key <auto|regex>` (group by tool family inferred from `cmd`)

**Behavior:** Group by family; stratify by OS/shell/language; write splits.

**Done when:** Leakage check passes (no family overlap); summary printed.

---

### T4) `train_unsloth.py`

**Goal:** Fine-tune with Unsloth QLoRA + automatic multi-GPU FSDP.

**Inputs:** `configs/train_sft.yaml`

**Required config keys:**

```yaml
model_name: Qwen/Qwen2.5-0.5B-Instruct  # or Qwen2.5-0.5B
quantization: qlora
bf16: true
seq_len: 2048
packing: true
optimizer: { name: adamw, lr: 2.0e-4, weight_decay: 0.1, warmup_ratio: 0.03 }
scheduler: cosine
lora: { r: 32, alpha: 32, target_modules: all-linear }
fsdp: { enable: true, shard_grad_op: true, cpu_offload: false }
gradient_checkpointing: true
logging_steps: 20
eval_steps: 200
save_steps: 1000
max_steps: 20000
train_file: data/splits/train.jsonl
val_file: data/splits/val.jsonl
output_dir: outputs/qwen0.5b-shell
```

**Behavior:**

* Auto-detect GPUs via `CUDA_VISIBLE_DEVICES`; enable FSDP when `fsdp.enable`.
* Use **NF4** 4-bit quant for QLoRA; compute in bf16.
* Format examples as chat messages:

  * system: “You convert natural language into safe, single-line shell commands in JSON.”
  * user: `input`
  * response: `output_json` (string)
* Save adapter and full checkpoints; log throughput and token counts.

**Done when:** Training runs multi-GPU with FSDP; best checkpoint saved; val loss/metrics logged.

---

### T5) vLLM serving

**Files:** `serve_vllm_base.sh`, `serve_vllm_ft.sh`

**Behavior:**

* Start two HTTP endpoints with identical decoding params.
* Base: `Qwen/Qwen2.5-0.5B(-Instruct)`.
* Fine-tuned: load base + LoRA adapter (**or** merged weights).
* Provide port numbers via env (`BASE_PORT`, `FT_PORT`).

**Done when:** Both endpoints respond; `/metrics` or healthcheck passes.

---

### T6) `eval_generate.py`

**Goal:** Query **both** endpoints on `data/splits/test.jsonl`.

**Inputs:** `--base_url --ft_url --in --out_dir --temperature 0.2 --top_p 0.95 --max_new_tokens 256`

**Behavior:** For each test item:

* Send consistent prompts (include `context` hints if present).
* Collect raw model outputs; attempt **strict JSON parse**; mark invalids.

**Outputs:** `preds_base.jsonl`, `preds_ft.jsonl`

**Done when:** 100% test items attempted; JSON validity stats printed.

---

### T7) `exec_sandbox.py`

**Goal:** Dry functional check in **Docker** sandbox (read-only, no network).

**Inputs:** `--preds <jsonl> --out <jsonl> --timeout 5 --mem 512m --cpus 1.0`

**Behavior:**

* Map `type` to images:

  * bash/zsh/fish → `ubuntu:<tag>` + shells installed.
  * powershell → `mcr.microsoft.com/powershell`.
  * cmd → optional (skip or WSL image; record as “not run”).
* Mount temp workspace read-only when possible; drop caps; seccomp default.
* If `requires_confirm==true`: **do not execute**; simulate success=unknown.
* Else: execute; record `{ok, exit_code, stderr_head, runtime_ms}`.

**Done when:** Produces functional results for all runnable shells; skips recorded.

---

### T8) `eval_score.py`

**Goal:** Compute metrics & report.

**Metrics:**

1. **Structural**: JSON validity %, required fields present, allowed `type`.
2. **Textual**: Exact-Match@cmd (normalized), token F1, edit distance.
3. **Functional**: ExecSuccess@dry, exit-code==0 rate, stderr length stats.
4. **Safety**:

   * Risk violation: `requires_confirm=false` but cmd matches risky patterns.
   * Over-caution: benign command with `requires_confirm=true`.

**Outputs:**

* Overall + breakdown by OS/shell + difficulty.
* Head-to-head table (Base vs FT) and **delta**.
* Markdown report in `out/report.md`.

**Done when:** Report generated with clear win/loss and confidence intervals (bootstrap OK).

---

## 6) Coding Standards & Dependencies

* Python ≥3.10; prefer `uv`/`pip-tools` or `conda` env.
* Use `unsloth`, `transformers`, `datasets`, `accelerate` (if needed), `docker` python SDK (or subprocess), `shellcheck`, `pwsh`.
* Provide **CLI help** (`--help`) and docstrings.
* All scripts must exit non-zero on failure; log to stdout.

## 7) Evaluation Protocol

* Decode with fixed params: `temperature=0.2`, `top_p=0.95`, `max_new_tokens=256`.
* Same prompts to both models; same stop rules.
* Do **not** repair outputs except to attempt JSON closing; if repaired, mark as **invalid** for structure metrics (but keep for text metrics with a flag).

## 8) Reproducibility

* Save:

  * `outputs/<run_id>/config_snapshot.yaml`
  * tokenizer + adapter
  * `inference.json` (decoding params)
  * `metrics.json`
* Log seeds, hardware, CUDA, driver, and unsloth version.

## 9) Milestones (for automation)

* **M0:** Synthesis v1 (≥10k), filters pass, splits done.
* **M1:** Train v1 (QLoRA+FSDP), checkpoints saved.
* **M2:** vLLM endpoints up; eval_generate done.
* **M3:** Sandbox functional eval; report shows FT ≥ Base on ≥3 metrics.
* **M4:** Add challenge test improvements; retrain v2; publish report.

---

### Quick Start Commands

```bash
# 1) Generate synthetic data to target OS mix
python scripts/synth_generate.py --seeds data/synthetic/raw_text \
  --out data/synthetic/samples/synth_v1.jsonl \
  --target_counts '{"linux":45,"macos":25,"windows":30}'

python scripts/filter_lint.py --in data/synthetic/samples/synth_v1.jsonl \
  --out data/synthetic/samples/synth_v1.clean.jsonl

python scripts/split_dataset.py --in data/synthetic/samples/synth_v1.clean.jsonl

# 2) Train with Unsloth (multi-GPU via FSDP)
python scripts/train_unsloth.py --config configs/train_sft.yaml

# 3) Serve base & finetuned
bash scripts/serve_vllm_base.sh
bash scripts/serve_vllm_ft.sh

# 4) Evaluate
python scripts/eval_generate.py --base_url http://localhost:8000 --ft_url http://localhost:8001 \
  --in data/splits/test.jsonl --out_dir outputs/eval_run1

python scripts/exec_sandbox.py --preds outputs/eval_run1/preds_ft.jsonl --out outputs/eval_run1/exec_ft.jsonl
python scripts/exec_sandbox.py --preds outputs/eval_run1/preds_base.jsonl --out outputs/eval_run1/exec_base.jsonl

python scripts/eval_score.py --gold data/splits/test.jsonl \
  --base outputs/eval_run1 --ft outputs/eval_run1
```
